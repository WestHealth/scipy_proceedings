:author: Haw-minn Lu
:email: hlu@westhealth.org
:institution: Gary and Mary West Health Institute
:bibliography: ourbib


.. class:: abstract

Most machine learning models, especially artificial neural networks, require numerical, not categorical data. We briefly describe the advantages and disadvantages of common encoding schemes. For example, one-hot encoding is commonly used for attributes with a few unrelated categories and word embeddings for attributes with many related categories (e.g., words). Neither is suitable for encoding attributes with many unrelated categories, such as diagnosis codes in healthcare applications. Application of one-hot encoding for diagnosis codes, for example, can result in extremely high dimensionality with low sample size problems or artificially induce machine learning artifacts, not to mention the explosion of computing resources needed. Quasi-orthonormal encoding (QOE) fills the gap. We briefly show how QOE compares to one-hot encoding. We provide example code of how to implement QOE using popular ML libraries such as Tensorflow and PyTorch and a demonstration of QOE to MNIST handwriting samples.

.. class:: keywords

   terraforming, desert, numerical perspective

============================================================
Quasi-orthonormal Encoding for Machine Learning Applications
============================================================

Introduction
------------


While most popular machine learning methods such as deep learning
require numerical data as input, categorical data is very common
practice. For example, a person's vitals could be a combination of both,
they could include height, weight (numerical) and gender, race
(categorical). The challenge is to convert the categorical data into a
vector of some sort. There is a good overview of most encoding
techniques at this blog
post :cite:`Hale2018`.
He further categorizes the coding techniques as *classic*, *contrast*,
and *Bayesian*. Notably, *word embeddings* are not mentioned. Both
contrast encoding and Bayesian encoding use the statistics of the data
to facilitate encoding. These two categories may be of use when more
statistical analysis is required, however there has not been widespread
adoption of these emcoding techniques for machine learning. These
encoding techiques can be found as part of the ``scikit-learn`` category
`encoding
package <https://contrib.scikit-learn.org/categorical-encoding/index.html>`__.

Though we will pursue a discussion of classic encodings and modern
derivatives, word embeddings deserve special mention. Word embeddings
are used to represent words, phrases or even entire documents as a
vector. Their goal is that similar meaning or concepts get mapped to
vectors that are close in the target vector space. Additionally, it is
adapted for encoding a large categorical feature (i.e., words) into a
relatively lower dimensional space.

Categorical Encodings
---------------------

Ordinal Encoding
~~~~~~~~~~~~~~~~

To begin our overview of fundamental encoding methods, we start with
Ordinal (Label) Encoding. Ordinal encoding is the simplest and perhaps
most naive approach encoding a categorical feature --- one simply
assigns a number to each member of a category. This is often how data
from surveys are encoded into spreadsheets for easy storage and
calculation of basic statistics. An associated data dictionary is used
to convert the values back and forth between a number and a category.
Take for example the case of gender, male could be encoded as 1 and
female as 2, with a data dictionary as follows:
``{'male': 1, 'female': 2}``

Suppose we have three categories of ethnic groups: White, Black, and
Asian. Under ordinal encoding, suppose What is encoded as 1, Black is
encoded as 2 and Asian is encoded as 3. If a machine learning
classification is somehow confused between Asian and White and decides
to split the difference and report the in-between value (2) which
encodes Black. The issue is that arbitrary gradation between 1 and 3
introduces a natural interpolation (2) that may be nonsense. Thus, the
natural ordering of the numbers imposes an ordered geometrical
relationship between the categories that does not apply.

Nonetheless there are situations where ordinal encoding makes sense. For
example, a 'rate your satisfaction' survey typically encodes five levels
(1) terrible, (2) acceptable (3) mediocre, (4) good, (5) excellent.

One Hot Encoding
~~~~~~~~~~~~~~~~

This is the most common encoding used in machine learning. One hot
encoding takes a category with cardinality :math:`N` and encodes each
categorical value with an :math:`N`-dimensional vector with a single '1'
and the remainder '0's. Take as an example encoding 5 makes of Japanese
Cars: Toyota, Honda, Subaru, Nissan, Mitsubishi. The following table
shows a comparison of coding between ordinal and one-hot:

+--------------+-----------+---------------+
| Make         | Ordinal   | One-Hot       |
+==============+===========+===============+
| Toyota       | 1         | (1,0,0,0,0)   |
+--------------+-----------+---------------+
| Honda        | 2         | (0,1,0,0,0)   |
+--------------+-----------+---------------+
| Subaru       | 3         | (0,0,1,0,0)   |
+--------------+-----------+---------------+
| Nissan       | 4         | (0,0,0,1,0)   |
+--------------+-----------+---------------+
| Mitsubishi   | 5         | (0,0,0,0,1)   |
+--------------+-----------+---------------+

The advantage is that one hot encoding doesn't induce an implicit
ordering or between categories. The primary disadvantage is that the
dimensionality of the problem has increased with corresponding increases
in complexity and computation (see `curse of
dimensionality <https://en.wikipedia.org/wiki/Curse_of_dimensionality>`__)
This easily leads to the high dimensionality low sample size (HDLSS)
situation, which is a problem for most machine learning methods.

Binary Encoding, Hash Encoding, BaseN Encoding
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Somewhere in between these two are *binary encoding*, *hash encoding*,
and *baseN* encoding. Binary encoding simply labels each category with a
unique binary code and converts the binary code to a vector. Using the
previous example of the Japanese car makes:

+--------------+-----------+-------------+---------------+
| Make         | Ordinal   | as Binary   | Binary Code   |
+==============+===========+=============+===============+
| Toyota       | 1         | 001         | (0,0,1)       |
+--------------+-----------+-------------+---------------+
| Honda        | 2         | 010         | (0,1,0)       |
+--------------+-----------+-------------+---------------+
| Subaru       | 3         | 011         | (0,1,1)       |
+--------------+-----------+-------------+---------------+
| Nissan       | 4         | 100         | (1,0,0)       |
+--------------+-----------+-------------+---------------+
| Mitsubishi   | 5         | 101         | (1,0,1)       |
+--------------+-----------+-------------+---------------+

Hash encoding assigns each category an ordinal value that is then
converted into a binary hash value that is encoded as an :math:`n`-tuple
in the same fashion as the binary encoding. You can view hash encoding
as binary encoding applied to the hashed ordinal value. Hash encoding
has several advantages. First, it is open ended so new categories can be
added later. Second, the resultant dimensionality can be much lower than
one-hot encoding. The chief disadvantage is that categories can collide
if two categories accidentally map into the same hash value. This is a
*hash collision* and must be fixed separately using a resolution
mechanism. A good treatment of hash coding can be found
`here <https://booking.ai/dont-be-tricked-by-the-hashing-trick-192a6aae3087>`__

Finally, baseN encoding is a generalization of binary encoding that uses
a number base other than 2 (binary). Below is an example of the Japanese
car makes using base 3,

+--------------+-----------+--------------+----------------+-------------------------+
| Make         | Ordinal   | as Ternary   | Ternary Code   | Balanced Ternary Code   |
+==============+===========+==============+================+=========================+
| Toyota       | 1         | 01           | (0,1)          | (0,1)                   |
+--------------+-----------+--------------+----------------+-------------------------+
| Honda        | 2         | 02           | (0,2)          | (0,-1)                  |
+--------------+-----------+--------------+----------------+-------------------------+
| Subaru       | 3         | 10           | (1,0)          | (1,0)                   |
+--------------+-----------+--------------+----------------+-------------------------+
| Nissan       | 4         | 11           | (1,1)          | (1,1)                   |
+--------------+-----------+--------------+----------------+-------------------------+
| Mitsubishi   | 5         | 12           | (1,2)          | (1,-1)                  |
+--------------+-----------+--------------+----------------+-------------------------+

A disadvantage of all three of these techniques is that while it does
reduce the dimension of the encoded feature, artificial geometric
relationships may creep in between unrelated categories. For example,
``(0.7,0.7)`` may be confused between Toyota and Honda or a weak Subaru
result, although this is still better than ordinal encoding.

Quasiorthonormal Encoding
~~~~~~~~~~~~~~~~~~~~~~~~~

Quasiorthonormal encoding (QOE) is a generalization of the one-hot
encoding and exploits the fact that in high-dimensional vector spaces,
two random vectors are generally almost orthogonal. The concept
originated with `Kůrková and
Kainen <https://ieeexplore.ieee.org/document/549073>`__ almost 25 years
ago. The principle is the same as one-hot encoding when you view the
encoded values as unit vectors as shown:

+--------------+-----------+------------------------+------------------------+
| Make         | Ordinal   | One-Hot                | QOE                    |
+==============+===========+========================+========================+
| Toyota       | 1         | :math:`\mathbf{u}_1`   | :math:`\mathbf{q}_1`   |
+--------------+-----------+------------------------+------------------------+
| Honda        | 2         | :math:`\mathbf{u}_2`   | :math:`\mathbf{q}_2`   |
+--------------+-----------+------------------------+------------------------+
| Subaru       | 3         | :math:`\mathbf{u}_3`   | :math:`\mathbf{q}_3`   |
+--------------+-----------+------------------------+------------------------+
| Nissan       | 4         | :math:`\mathbf{u}_4`   | :math:`\mathbf{q}_4`   |
+--------------+-----------+------------------------+------------------------+
| Mitsubishi   | 5         | :math:`\mathbf{u}_5`   | :math:`\mathbf{q}_5`   |
+--------------+-----------+------------------------+------------------------+

where :math:`\mathbf{u}_i` are unit vectors in :math:`\mathbb{R}^5` and
:math:`{\mathbf{q}_i}` are a set of quasiorthogonal vectors in a
dimension less than 5. The set :math:`{\mathbf{q}_i}` is
quasiorthonormal if each :math:`\mathbf{q}_i` is normal
(:math:`||\mathbf{q}_i||=1`) and
:math:`|\mathbf{q}_i\cdot \mathbf{q}_j|<\epsilon` for some small
:math:`\epsilon` value and for :math:`i\ne j`. In otherwords, if
:math:`i` and :math:`j` are distinct :math:`\mathbf{q}_i` and
:math:`\mathbf{q}_j` are almost perpendicular.

We will explore more about QOE in the next article.

Summary
~~~~~~~

Although most machine learning methods require numerical data,
categorical data is very common in certain areas, such as medical data
science or survey analysis. We reviewed the most popular encoding
methods to make this kind of data tractable for machine learning, but
each of them has drawbacks in computation or interpretation that can
lead to confusing downstream results, if not handled correctly at the
outset.

Introduction
------------

Quasiorthonormal encoding (QOE) is a generalization of the one-hot
encoding and exploits the fact that in high dimensional vector spaces,
two random vectors are almost always orthogonal. The concept originated
with `Kůrková and
Kainen <https://ieeexplore.ieee.org/document/549073>`__ almost 25 years
ago. The principle is the same as one-hot encoding when you view the
encoded values as unit vectors as shown:

+--------------+-----------+------------------------+------------------------+
| Make         | Ordinal   | One-Hot                | QOE                    |
+==============+===========+========================+========================+
| Toyota       | 1         | :math:`\mathbf{u}_1`   | :math:`\mathbf{q}_1`   |
+--------------+-----------+------------------------+------------------------+
| Honda        | 2         | :math:`\mathbf{u}_2`   | :math:`\mathbf{q}_2`   |
+--------------+-----------+------------------------+------------------------+
| Subaru       | 3         | :math:`\mathbf{u}_3`   | :math:`\mathbf{q}_3`   |
+--------------+-----------+------------------------+------------------------+
| Nissan       | 4         | :math:`\mathbf{u}_4`   | :math:`\mathbf{q}_4`   |
+--------------+-----------+------------------------+------------------------+
| Mitsubishi   | 5         | :math:`\mathbf{u}_5`   | :math:`\mathbf{q}_5`   |
+--------------+-----------+------------------------+------------------------+

where :math:`\mathbf{u}_i` are unit vectors in :math:`\mathbb{R}^5` and
:math:`{\mathbf{q}_i}` are a set of quasiorthogonal vectors in a
dimension less than 5. The set :math:`{\mathbf{q}_i}` is
quasiorthonormal if each :math:`\mathbf{q}_i` is normal
(:math:`||\mathbf{q}_i||=1`) and
:math:`|\mathbf{q}_i\cdot \mathbf{q}_j|<\epsilon` for some small
:math:`\epsilon` value and for :math:`i\ne j`. In otherwords, if
:math:`i` and :math:`j` are distinct :math:`\mathbf{q}_i` and
:math:`\mathbf{q}_j` are almost perpendicular.

By relaxing the condition that the encoded values be orthogonal many
more codes can be used in a smaller number of dimensions. A similar
principle to this is used in areas such as Code Division Multiple Access
(CDMA) that is used in mobile telephony. In lay terms, if you allow for
almost orthogonal vectors rather than strictly orthogonal, you can fit
more vectors into the same vector space.

Some advantages to QOE include a reduction of dimensionality over that
of using one-hot encoding thus limiting effects of the "curse of
dimensionality" or the problem of high dimension low sample size
(HDLSS). The advantage over other encodings such as binary, hash, etc.
is that it does not induce artificial geometric relationships that can
cause downlstream bias in the results because each label in a category
remains mathematically orthogonal to the other labels.

While Kůrková and Kainen proposed this type of encoding almost 25 years
ago, machine learning has advanced since then so we include a short
recap of the relevant issues next.

One Hot Encoding Recap
----------------------

In machine learning, the typical aspects of one hot encoding maps a
variable with :math:`n` categories into a set of unit vectors in a
:math:`n`-dimensional space: :math:`L=\{l_i\}` for :math:`i=1\ldots n`,
then the one hot encoding :math:`\mathbb{1}_L:L \mapsto \mathbb{R}^n`
given by :math:`l_i \mapsto \mathbf{u}_i` where :math:`\mathbf{u}_i` is
an orthonormal basis in :math:`\mathbb{R}^n`. The simplest basis used is
:math:`\mathbf{u}_i = (0,0,\ldots, 1, 0,\ldots, 0)` where the :math:`1`
is in the :math:`i`\ th position which is know as the *standard basis*
for :math:`\mathbb{R}^n`.

Mapping of a vector back to the original category uses the *argmax*
function, so for a vector :math:`\mathbf{z}`,
:math:`\mathrm{argmax}\mathbf(z) = i` where :math:`z_i>z_j` for all
:math:`j\ne i` and the vector :math:`\mathbf{z}` decodes to
:math:`l_{\mathrm{argmax}(\mathbf{z})}`. Of course, the argmax function
is not easily differentiable which problems in ML learning algorithms
that require derivatives. To fix this, a *softer* version is used called
the *softargmax* or now as simply *softmax* and is defined as follows:

.. math:: \mathrm{softmax}(\mathbf{z})_i={e^{z_i}\over \sum_{j=1}^n e^{z_j}}

for :math:`i=1,2,\ldots,n` and
:math:`{\bf z}=(z_1, z_2,\ldots, z_n) \in \mathbb{R}^n` where
:math:`\mathbf{z}` is the vector being decoded. The softmax function
decodes a one-hote encoded vector into a probability density function
which enables application of negative log likelihood loss functions or
cross entropy losses.

We can generalize the one-hot encoding formulation to use an arbitrary
orthonormal basis :math:`{\mathbf{u}_i}`. To decode a one-hot encoded
value in this framework, we would take
:math:`i = \mathrm{argmax}(\mathbf{z}\cdot\mathbf{u}_1,\mathbf{z}\cdot\mathbf{u}_2,\ldots,\mathbf{z}\cdot\mathbf{u}_n)`.
This reduces to :math:`\mathrm{argmax}\mathbf(z)` for the standard
basis. Thus, the softmax function can be expressed as the following,

.. math:: \mathrm{softmax}({\bf z})_i={e^{{\bf z}\cdot {\bf u}_i}\over \sum_{j=1}^n e^{{\bf z}\cdot {\bf u}_j}}.

Quasiorthogonality
------------------

Given an :math:`\epsilon` two vectors :math:`{\bf x}` and
:math:`{\bf y}` are said to be *quasiorthogonal* if
:math:`{|{\bf x}\cdot {\bf y}|\over \|{\bf x}\| \|{\bf y}\|}<\epsilon`.
This extends the orthogonality principle by allowing the inner product
to not exactly equal zero. As an extension, we can define a
quasiorthonormal *basis* by a set of normal vectors
:math:`\{{\bf q}_i\}` for :math:`i=1,\ldots,K` such that
:math:`|{\bf q}_i\cdot {\bf q}_j< \epsilon |` and
:math:`||{\bf q}_i||=1`, for all :math:`i,j\in\{1,\ldots,K\}`, where in
principle for large enough :math:`n`, :math:`K\gg n`. `Kainen and
Kůrková <https://www.cs.cas.cz/~vera/publications/books/C1.pdf>`__
derived a lower bound for :math:`K` as a function of :math:`\epsilon`
and :math:`n`. Namely,

.. math:: K \ge e^{n\epsilon^2}.

Quasiorthonormal Encoding
-------------------------

Briefly, quasiorthonormal encoding simply substitutes a quasiorthonormal
basis :math:`\{{\bf q}_i\}` for the orthonormal basis
:math:`\{{\bf u}_i\}` used above. So more formally, given a
quasiorthonormal basis, we can define a QOE for a set :math:`L=\{l_i\}`
by :math:`{\mathbb q}(l_i)= {\bf q}_i`. Decoding under QOE would use the
following formula for decoding :math:`\mathbf{z}`:

.. math::

   i =
   \mathrm{argmax}(\mathbf{z}\cdot\mathbf{q}_1,\mathbf{z}\cdot\mathbf{q}_2,\ldots,\mathbf{z}\cdot\mathbf{q}_n)

The analogous softmax function, let's call it *qsoftmax*, would be
expressed as

.. math::

   \mathrm{qsoftmax}({\bf z})_i={e^{{\bf z}\cdot {\bf q}_i}\over \sum_{j=1}^K
   e^{{\bf z}\cdot {\bf q}_j}}

The only real difference in the formulation is that while still
operating in :math:`{\mathbb R}^n` we are encoding :math:`K>n` labels.

Implementation
--------------

To facilitate modern vectorized computation packages such as ``numpy``
and ``tensorflow``, we define the following :math:`n\times K` *change of
coordinates* matrix

.. math::

   \mathbf{Q}=  \left[\begin{matrix} 
   \bigg| & \bigg| & &\bigg | \\ 
   \mathbf{q}_1 & \mathbf{q}_2 & \cdots & \mathbf{q}_K \\
   \bigg| & \bigg| & &\bigg | \end{matrix}\right].

that transforms between the QOE space and the one hot encoding space. So
given a argmax or softmax function, we can express the quasiorthonormal
variant as follows

.. math:: \mathrm{qargmax}(\mathbf{z}) = \mathrm{argmax}(\mathbf{Qz})

and

.. math:: \mathrm{qsoftmax}(\mathbf{z}) = \mathrm{softmax}(\mathbf{Qz}).

This facilitates optimized functions such as ``softmax`` in libraries
like ``tensorflow`` and using the above matrix enables quick
implementation of QOE into these packages. In the examples below, we use
``tensorflow`` to test the effectiveness of using QOE over one-hot
encoding.

Construction of an Quasiorthonormal set
---------------------------------------

It is difficult find explicit constructions of quasiorthonormal sets in
the literature. Several methods are mentioned by
`Kainen <https://www.researchgate.net/publication/303520147>`__, but
these constructions are somewhat theoretical and hard for the lay person
to follow. There are a number of combinatorial problems related such as
spherical codes and Steiner Triple Systems, which strive to find optimal
solutions. As a practical matter, we only need to find fast suboptimal
solutions so we can use `spherical
codes <http://neilsloane.com/packings/index.html>`__. Spherical codes
try to find a set of points on the :math:`n`-dimensional hypersphere
such that the minimum distance between two points is maximized. In most
constructions of spherical codes, a given point's antipodal point is
also in that code set. So in order to get a quasiorthogonal set, for
each pair of antipodal points, only one element of the pair is selected.

Experiment and Demonstration
----------------------------

As an initial experiment, we applied QOE to classification of the MNIST
handwriting dataset, using the 60000 training examples with 10000 test
examples. As there are 10 categories, we needed sets of quasiorthonormal
bases with 10 elements. We took the spherical code for 24 points in
4-dimensions, giving us 12 quasi-orthogonal vectors. The maximum
pairwise dot product was 0.5 leading to an angle of 60\ :math:`^\circ`.
We also took the spherical code for 56 points in 7-dimensions, giving 28
quasi-orthogonal vectors. The maximum pairwise dot product was .33
leading to an angle of a little over 70\ :math:`^\circ`

We used a hidden layer with 64 units with a ReLU activation function.
Next there is a 20% dropout layer to mitigate overtraining, then an
output layer whose width depends on the encoding used.

As a preliminary, we define a ``qsoftmax`` metafunction:

.. code:: python

    def qsoftmax(basis):
        def func(x):
            qx = tf.matmul(tf.constant(basis),x,transpose_b=True)        
            return tf.nn.softmax(tf.transpose(qx))
        return func

which takes a quasiorthogonal basis and returns the quasiorthogonal
softmax function based on the basis. The various transpose operations
are necessary to conform the inputs and outputs to the shape provided
and required by the tensorflow model.

For ``tensorflow`` and ``keras``, the base network is the following,

.. code:: python

    normal_model = tf.keras.models.Sequential([
      tf.keras.layers.Flatten(input_shape=(28, 28)),
      tf.keras.layers.Dense(64, activation=tf.nn.relu),
      tf.keras.layers.Dropout(0.2),
      tf.keras.layers.Dense(10)
      tf.keras.layers.Activation(tf.nn.softmax)
    ])

We used a separate ``Activation`` layer to make the ``qsoftmax``
clearer.

As a sanity test, you can implement the following:

.. code:: python

    sanity_model = tf.keras.models.Sequential([
      tf.keras.layers.Flatten(input_shape=(28, 28)),
      tf.keras.layers.Dense(64, activation=tf.nn.relu),
      tf.keras.layers.Dropout(0.2),
      tf.keras.layers.Dense(10)
      tf.keras.layers.Lambda(qsoftmax(numpy.identity(10,dtype=numpy.float32)))
    ])

This should function identically as the reference model because it tests
that the qsoftmax function operates as expected (which it does for us)
since applying the identity matrix to ``qsoftmax`` merely converts it to
the standard ``softmax`` function. This is useful for troubleshooting if
you have difficulty.

For the two QOE experiments we labeled the bases for the two
quasiorthonormal sets ``base4`` and ``base7`` and only took the first 10
vectors. We used the following additional test models

.. code:: python

    basis4_model = tf.keras.models.Sequential([
      tf.keras.layers.Flatten(input_shape=(28, 28)),
      tf.keras.layers.Dense(64, activation=tf.nn.relu),
      tf.keras.layers.Dropout(0.2),
      tf.keras.layers.Dense(4),
      tf.keras.layers.Lambda(qsoftmax(basis4))
    ])
    basis7_model = tf.keras.models.Sequential([
      tf.keras.layers.Flatten(input_shape=(28, 28)),
      tf.keras.layers.Dense(64, activation=tf.nn.relu),
      tf.keras.layers.Dropout(0.2),
      tf.keras.layers.Dense(7),
      tf.keras.layers.Lambda(qsoftmax(basis7))
    ])

The following table is the mean of the accuracy over three training runs
of the validation data with training data in parentheses

+--------------------+--------------------+--------------------+--------------------+
| Number of Epochs   | One Hot Encoding   | 7-Dimensional QO   | 4-Dimensional QO   |
+====================+====================+====================+====================+
| 10                 | 97.53% (97.30%)    | 97.24% (96.94%)    | 95.65% (95.15%)    |
+--------------------+--------------------+--------------------+--------------------+
| 20                 | 97.68% (98.02%)    | 97.49% (97.75%)    | 95.94% (96.15%)    |
+--------------------+--------------------+--------------------+--------------------+

The seven dimensional quasiorthogonal basis (at 70\ :math:`^\circ`),
performs nearly as well as the one-hot encoded version whereas the four
dimensional quasiorthogonal basis (at 60\ :math:`^\circ`) case did not
perform as well. Though 95% vs 98% percent may not seem significant, for
the MNIST handwriting dataset, this relatively small change in
performance is how these classification algorithms are judged.

Spherical Encoding
------------------

One key to the effectiveness of an encoding method is how well
categorical values can be decoded in an noisy environment. Orthogonality
provides excellent decodability especially using the softmax function,
but this comes at the cost of the number of dimensions used for the
encoding. Quasiorthoganility with the QO softmax function performs less
effectively at decoding than orthogonality with the softmax, but reduces
dimensionality.

A curious observation is that if you examine the effect of the softmax
function on negative values along one of the basis vectors, the softmax
function severely attenuates it. Therefore, one can construct an
orthogonal code using both the 1 and -1 values so for standard unit
vectors, you could encode categorical values to
:math:`\{\mathbf{u}_i, -\mathbf{u}_i,\}`, reducing the dimensionality by
half. Of course nothing comes for free, if a prediction gets confused
between two antipodal unit vectors, the result could be that they cancel
out and allow the noise to dictate the resulting category. By contrast,
for one-hot encoding, the result would get decoded as one of the two
possible values.

With this risk in mind, we can further extend the idea to a
quasiorthogonal basis by adding the antipodal vectors for each vector in
the basis. The result not only doubles the number of vectors that can be
used for encoding, it reduces the problem of finding a basis to that of
finding spherical codes.

Finally, we tested how effective this types of coding is by using an
orthogonal basis in 5 dimensions and adding the antipodal unit vector to
produce a set of 10 vectors.

The model tested becomes:

.. code:: python

    basis5_model = tf.keras.models.Sequential([
      tf.keras.layers.Flatten(input_shape=(28, 28)),
      tf.keras.layers.Dense(64, activation=tf.nn.relu),
      tf.keras.layers.Dropout(0.2),
      tf.keras.layers.Dense(5),
      tf.keras.layers.Lambda(qsoftmax(basis5))
    ])

In addition, we ran a test using the 3 dimension 10 vector spherical
code provided `here <http://neilsloane.com/packings/index.html>`__. The
accuracy is shown in the following table with training accuracy in
parentheses.

+--------------------+--------------------+--------------------+--------------------+
| Number of Epochs   | One Hot Encoding   | 5-Dimensional SC   | 3-Dimensional SC   |
+====================+====================+====================+====================+
| 10                 | 97.53% (97.30%)    | 96.51% (96.26%)    | 95.37% (94.83%)    |
+--------------------+--------------------+--------------------+--------------------+
| 20                 | 97.68% (98.02%)    | 96.82% (97.11%)    | 95.74% (95.83%)    |
+--------------------+--------------------+--------------------+--------------------+

In this case, the 5-dimensional spherical codes performed close to the
one-hot encoding by not as closely as the 7-dimension QO codes. The
3-dimensional spherical codes performed on par with the 4-dimensional QO
codes.

Conclusion
~~~~~~~~~~

While the extreme dimensionality reduction from 10 to 4 or 10 to 3 did
not yield comparable performance to one-hot encoding. More modest
reductions such as 10 to 7 and 10 to 5 did. It is worth considering that
quasiorthogonal or spherical codes are much harder to find in low
dimensions. One should note that, though we went from 10 to 7
dimensions, we did not fully exploit the space spanned by the
quasiorthogonal vector set. Otherwise, we would likely have had the
similar results if the categorical labels had a cardinality of 28 rather
than 10. Furthermore, as the target encoded space approaches 20 or 30
dimensions, we have the ability to encode 100,000 or even a million
labels. So for high cardinality categories, QO encoding and spherical
encoding provide an efficient categorical encoding while controlling the
curse of dimensionality.

References
----------
